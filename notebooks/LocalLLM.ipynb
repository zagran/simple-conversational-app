{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-21T18:18:24.683341Z",
     "start_time": "2024-02-21T18:18:22.696334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (0.2.44)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (from llama-cpp-python) (4.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (from llama-cpp-python) (1.26.4)\r\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (from llama-cpp-python) (5.6.3)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (from llama-cpp-python) (3.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zagran/workspace/simple-conversational-app/.venv/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T00:29:55.654691Z",
     "start_time": "2024-02-20T00:29:55.445171Z"
    }
   },
   "id": "cba06662d887383e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "verbose = False\n",
    "MODEL_PATH = \"../models/synthia-7b-v2.0-16k.Q2_K.gguf\"\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=MODEL_PATH,\n",
    "    # max tokens the model can account for when processing a response\n",
    "    # make it large enough for the question and answer\n",
    "    n_ctx=4096,\n",
    "    # number of layers to offload to the GPU\n",
    "    # GPU is not strictly required but it does help\n",
    "    n_gpu_layers=32,\n",
    "    # number of tokens in the prompt that are fed into the model at a time\n",
    "    n_batch=1024,\n",
    "    # use half precision for key/value cache; set to True per langchain doc\n",
    "    f16_kv=True,\n",
    "    verbose=verbose,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T00:29:56.019681Z",
     "start_time": "2024-02-20T00:29:55.655394Z"
    }
   },
   "id": "b6f26c2b49e3d670",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This question has been asked by people for thousands of years. It’s a question that can be answered in many different ways, depending on your beliefs and values. But ultimately, the meaning of life comes down to what you make of it.\n",
      "\n",
      "For some people, the meaning of life is to find happiness and enjoyment in every moment. For others, it’s about finding purpose and making a difference in the world. And still for others, it’s about finding peace and contentment within oneself.\n",
      "\n",
      "Ultimately, the meaning of life is up to each individual to decide for themselves. There is no one-size-fits-all answer to this question. So take some time to reflect on what makes you happy, what gives you purpose, and what brings you peace. Then, go out and live your best life!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the meaning of life?\"\n",
    "\n",
    "output = llm.invoke(\n",
    "        question,\n",
    "        max_tokens=4096,\n",
    "        temperature=0.2,\n",
    "        # nucleus sampling (mass probability index)\n",
    "        # controls the cumulative probability of the generated tokens\n",
    "        # the higher top_p the more diversity in the output\n",
    "        top_p=0.1\n",
    "    )\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T00:32:56.570759Z",
     "start_time": "2024-02-20T00:32:45.040483Z"
    }
   },
   "id": "df83ee8e961934de",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T00:30:05.692598Z",
     "start_time": "2024-02-20T00:30:05.691164Z"
    }
   },
   "id": "5e558a63fde9bfb6",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
